{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d42475ec",
   "metadata": {},
   "source": [
    "# Using Azure PostgreSQL as the Unified Data Layer for an Agentic Framework with LangGraph\n",
    "\n",
    "This notebook demonstrates an end-to-end pattern for using **Azure Database for\n",
    "PostgreSQL** as the *single* persistence + retrieval layer for an AI agent graph\n",
    "built with **LangGraph**.\n",
    "\n",
    "We consolidate:\n",
    "\n",
    "- Durable agent state / checkpoints\n",
    "- Episodic & semantic memory\n",
    "- Chat history (multi-session)\n",
    "- Vector storage (pgvector) for Retrieval Augmented Generation (RAG)\n",
    "\n",
    "## High-Level Architecture\n",
    "\n",
    "| Concern                 | Postgres Object                 | Notes                                              |\n",
    "| ----------------------- | ------------------------------- | -------------------------------------------------- |\n",
    "| Agent graph checkpoints | checkpoints                     | Serialized LangGraph state per thread / step       |\n",
    "| Chat history            | checkpoints                     | Ordered log of messages (roles, content, metadata) |\n",
    "| Long-term memory        | store                           | Key-value or typed memory entries                  |\n",
    "| Knowledge / docs        | documents (content + embedding) | Vector similarity over pgvector                    |\n",
    "| Embeddings index        | documents.embedding (vector)    | Uses pgvector `vector` type                        |\n",
    "\n",
    "We'll walk through: installation, connection, schema DDL, helper data-access functions, LangGraph integration, vector similarity search, and a retrieval-augmented interaction.\n",
    "\n",
    "## ⚠️ Prerequisites\n",
    "\n",
    "1. An Azure Database for PostgreSQL Flexible Server (v14+ recommended) with pgvector enabled.\n",
    "2. `pgvector` extension installed (we show how).\n",
    "3. API key for an embedding + chat model (Azure OpenAI or OpenAI).\n",
    "\n",
    "Set environment variables before running (example):\n",
    "\n",
    "```shell\n",
    "export AZURE_PG_HOST=your-server.postgres.database.azure.com\n",
    "export AZURE_PG_DB=agentdb\n",
    "export AZURE_PG_USER=agent_user\n",
    "export AZURE_PG_PASSWORD='YOUR_SECRET'  # consider Azure Key Vault in production\n",
    "export OPENAI_API_KEY=...              # or Azure OpenAI specifics\n",
    "# For Azure OpenAI also set: AZURE_OPENAI_ENDPOINT, AZURE_OPENAI_API_KEY, AZURE_OPENAI_DEPLOYMENT\n",
    "```\n",
    "\n",
    "> Performance tip: Co-locate compute (where LangGraph runs) and your Azure\n",
    "  Postgres server in the same region to minimize latency. Use connection pooling\n",
    "  (e.g. PgBouncer / Azure Flexible Server built-in) for concurrency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc20a2b9",
   "metadata": {},
   "source": [
    "## Import Necessary Modules and Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "799aff90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If environment variables from .env are not loaded/read automatically when the\n",
    "# notebook kernel is (re)started (default behavior with Visual Studio Code and\n",
    "# uv), then you can manually load them using the following code:\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18ff3e7",
   "metadata": {},
   "source": [
    "## Connect to Azure PostgreSQL\n",
    "\n",
    "We'll define the connection string or EntraID. Use SSL (`sslmode=require`) which\n",
    "Azure enforces by default. For production, prefer a connection pool (e.g.,\n",
    "PgBouncer) and secrets from Azure Key Vault instead of raw env vars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2436ea75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from langchain_azure_postgresql import (\n",
    "    AzurePGConnectionPool,\n",
    "    ConnectionInfo,\n",
    "    BasicAuth,\n",
    "    Extension,\n",
    "    create_extensions,\n",
    "    SSLMode,\n",
    ")\n",
    "from langchain_core.documents import Document\n",
    "import numpy as np\n",
    "from pgvector.psycopg import register_vector\n",
    "from psycopg import Connection\n",
    "from psycopg.rows import dict_row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9e109f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result=(1,)\n"
     ]
    }
   ],
   "source": [
    "PGHOST = os.getenv(\"PGHOST\", \"localhost\")\n",
    "connection_pool = AzurePGConnectionPool(\n",
    "    azure_conn_info=ConnectionInfo(\n",
    "        host=PGHOST,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Test that the connection pool works\n",
    "connection_pool.wait()\n",
    "\n",
    "with connection_pool.connection() as conn, conn.cursor() as cur:\n",
    "    cur.execute(\"select 1 as response\")\n",
    "    result = cur.fetchone()\n",
    "    assert result is not None\n",
    "    print(f\"{result=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd6de65",
   "metadata": {},
   "source": [
    "### Enable pgvector and DiskANN Extension (if not already)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d865957",
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_connection(conn: Connection) -> None:\n",
    "    \"\"\"Configure the PostgreSQL connection (as received from the pool).\n",
    "\n",
    "    `PostgresSaver` requires that the connection is initialized with a\n",
    "    `dict_row` row factory and set to auto-commit mode (cf. the docs).\n",
    "\n",
    "    We will make sure `pg_diskann` is created (together with `vector`) when the\n",
    "    connection is given (via `create extension if not exists pg_diskann cascade`).\n",
    "\n",
    "    Finally, we will register pgvector loaders and dumpers.\n",
    "\n",
    "    :param conn: The PostgreSQL connection to configure.\n",
    "    :type conn: Connection\n",
    "    \"\"\"\n",
    "    conn.autocommit = True\n",
    "    conn.row_factory = dict_row\n",
    "    create_extensions(conn, [Extension(ext_name=\"pg_diskann\", cascade=True)])\n",
    "    register_vector(conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b98e31b",
   "metadata": {},
   "source": [
    "### Using EntraID and Azure Postgres LangChain\n",
    "The following sections contain functions that set up LangChain to use Microsoft Entra authentication.\n",
    "\n",
    "To sign in to Azure, ensure that you have the [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli) installed. Run the following command in your terminal:\n",
    "\n",
    "```bash\n",
    "az login\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "99b51889",
   "metadata": {},
   "outputs": [],
   "source": [
    "PGHOST = os.getenv(\"PGHOST\", \"localhost\")\n",
    "\n",
    "connection_pool = AzurePGConnectionPool(\n",
    "    azure_conn_info=ConnectionInfo(\n",
    "        host=PGHOST\n",
    "    ),\n",
    "    configure=configure_connection,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f8015b",
   "metadata": {},
   "source": [
    "# Using Postgres as a vector store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2990e5a6",
   "metadata": {},
   "source": [
    "## Set up Azure OpenAI embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1b6cbc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureOpenAIEmbeddings, OpenAIEmbeddings\n",
    "\n",
    "model = \"text-embedding-3-small\"\n",
    "\n",
    "embedding = (\n",
    "    AzureOpenAIEmbeddings(model=model)\n",
    "    if os.getenv(\"AZURE_OPENAI_API_KEY\") is not None\n",
    "    else OpenAIEmbeddings(model=model)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6a0c08ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(query_embedding)=<class 'list'>, len(query_embedding)=1536\n"
     ]
    }
   ],
   "source": [
    "# Test that embedding generation works\n",
    "# For instance, OpenAI models have unit-norm embedding vectors\n",
    "\n",
    "query_embedding = embedding.embed_query(\"Hello World\")\n",
    "assert np.isclose(np.linalg.norm(query_embedding), 1, atol=1e-4)\n",
    "\n",
    "print(f\"{type(query_embedding)=}, {len(query_embedding)=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0f9ade",
   "metadata": {},
   "source": [
    "## Set Up Vector Store\n",
    "\n",
    "We define a normalized schema. Adjust dimensions to match the embedding model you use (e.g. 1536 for `text-embedding-3-small`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8e2d8ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_azure_postgresql import AzurePGVectorStore\n",
    "\n",
    "table_name = \"langgraph\"\n",
    "\n",
    "# Get a connection from the pool to pass to the vector store.\n",
    "# Ideally, this should be done in a context manager to ensure proper cleanup.\n",
    "# For this notebook, though, we'll just manually release the connection.\n",
    "conn_vectorstore = connection_pool.getconn()\n",
    "\n",
    "vector_store = AzurePGVectorStore(\n",
    "    embedding=embedding, connection=conn_vectorstore, table_name=table_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5ee64ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "    Document(\n",
    "        page_content=\"there are cats in the pond\",\n",
    "        metadata={\"doc_id\": 1, \"location\": \"pond\", \"topic\": \"animals\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"ducks are also found in the pond\",\n",
    "        metadata={\"doc_id\": 2, \"location\": \"pond\", \"topic\": \"animals\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"fresh apples are available at the market\",\n",
    "        metadata={\"doc_id\": 3, \"location\": \"market\", \"topic\": \"food\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"the market also sells fresh oranges\",\n",
    "        metadata={\"doc_id\": 4, \"location\": \"market\", \"topic\": \"food\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"the new art exhibit is fascinating\",\n",
    "        metadata={\"doc_id\": 5, \"location\": \"museum\", \"topic\": \"art\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"a sculpture exhibit is also at the museum\",\n",
    "        metadata={\"doc_id\": 6, \"location\": \"museum\", \"topic\": \"art\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"a new coffee shop opened on Main Street\",\n",
    "        metadata={\"doc_id\": 7, \"location\": \"Main Street\", \"topic\": \"food\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"the book club meets at the library\",\n",
    "        metadata={\"doc_id\": 8, \"location\": \"library\", \"topic\": \"reading\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"the library hosts a weekly story time for kids\",\n",
    "        metadata={\"doc_id\": 9, \"location\": \"library\", \"topic\": \"reading\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"a cooking class for beginners is offered at the community center\",\n",
    "        metadata={\"doc_id\": 10, \"location\": \"community center\", \"topic\": \"classes\"},\n",
    "    ),\n",
    "]\n",
    "\n",
    "uuids = vector_store.add_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd656e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* [SIM=0.645530] the new art exhibit is fascinating [{'topic': 'art', 'doc_id': 5, 'location': 'museum'}]\n",
      "* [SIM=0.645548] the new art exhibit is fascinating [{'topic': 'art', 'doc_id': 5, 'location': 'museum'}]\n",
      "* [SIM=0.645548] the new art exhibit is fascinating [{'topic': 'art', 'doc_id': 5, 'location': 'museum'}]\n"
     ]
    }
   ],
   "source": [
    "results = vector_store.similarity_search_with_score(query=\"something creative that will inspire\", k=3)\n",
    "for doc, score in results:\n",
    "    print(f\"* [SIM={score:3f}] {doc.page_content} [{doc.metadata}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e7f082",
   "metadata": {},
   "source": [
    "## Update items in the vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c714068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['9629a3aa-2dab-44ca-a833-29bc2491e34f']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = [\n",
    "    Document(\n",
    "        page_content=\"Updated - a cooking class for beginners is offered at the community center\",\n",
    "        metadata={\"doc_id\": 11, \"location\": \"community center\", \"topic\": \"classes\"},\n",
    "    )\n",
    "]\n",
    "vector_store.add_documents(docs, ids=[uuids[-1]], on_conflict_update=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a48e668",
   "metadata": {},
   "source": [
    "## Delete items from the vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6fbe224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.delete(ids=[uuids[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd398531",
   "metadata": {},
   "source": [
    "### Direct query\n",
    "You can perform a simple similarity search as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4063b99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_azure_postgresql import FilterCondition, AndFilter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17f0a019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* there are cats in the pond [{'topic': 'animals', 'doc_id': 1, 'location': 'pond'}]\n",
      "* there are cats in the pond [{'topic': 'animals', 'doc_id': 1, 'location': 'pond'}]\n",
      "* the new art exhibit is fascinating [{'topic': 'art', 'doc_id': 5, 'location': 'museum'}]\n",
      "* the new art exhibit is fascinating [{'topic': 'art', 'doc_id': 5, 'location': 'museum'}]\n",
      "* the library hosts a weekly story time for kids [{'topic': 'reading', 'doc_id': 9, 'location': 'library'}]\n",
      "* the library hosts a weekly story time for kids [{'topic': 'reading', 'doc_id': 9, 'location': 'library'}]\n",
      "* fresh apples are available at the market [{'topic': 'food', 'doc_id': 3, 'location': 'market'}]\n",
      "* fresh apples are available at the market [{'topic': 'food', 'doc_id': 3, 'location': 'market'}]\n",
      "* a new coffee shop opened on Main Street [{'topic': 'food', 'doc_id': 7, 'location': 'Main Street'}]\n",
      "* a new coffee shop opened on Main Street [{'topic': 'food', 'doc_id': 7, 'location': 'Main Street'}]\n"
     ]
    }
   ],
   "source": [
    "results = vector_store.similarity_search(\n",
    "    \"kitty\",\n",
    "    k=10,\n",
    "    filter=FilterCondition(\n",
    "        column=\"(metadata->>'doc_id')::int\",\n",
    "        operator=\"in\",\n",
    "        value=[1, 3, 5, 7, 9],\n",
    "    ),\n",
    ")\n",
    "\n",
    "for doc in results:\n",
    "    print(f\"* {doc.page_content} [{doc.metadata}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf2d461e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* there are cats in the pond [{'topic': 'animals', 'doc_id': 1, 'location': 'pond'}]\n",
      "* there are cats in the pond [{'topic': 'animals', 'doc_id': 1, 'location': 'pond'}]\n"
     ]
    }
   ],
   "source": [
    "results = vector_store.similarity_search(\n",
    "    \"ducks\",\n",
    "    k=10,\n",
    "    filter=AndFilter(\n",
    "        AND=[\n",
    "            FilterCondition(\n",
    "                column=\"(metadata->>'doc_id')::int\",\n",
    "                operator=\"in\",\n",
    "                value=[1, 5, 2, 9],\n",
    "            ),\n",
    "            FilterCondition(\n",
    "                column=\"metadata->>'location'\",\n",
    "                operator=\"in\",\n",
    "                value=[\"pond\", \"market\"],\n",
    "            ),\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "\n",
    "for doc in results:\n",
    "    print(f\"* {doc.page_content} [{doc.metadata}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e98eb7",
   "metadata": {},
   "source": [
    "If you want to execute a similarity search and receive the corresponding scores, you can run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d2001a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* [SIM=0.528251] there are cats in the pond [{'topic': 'animals', 'doc_id': 1, 'location': 'pond'}]\n"
     ]
    }
   ],
   "source": [
    "results = vector_store.similarity_search_with_score(query=\"cats\", k=1)\n",
    "for doc, score in results:\n",
    "    print(f\"* [SIM={score:3f}] {doc.page_content} [{doc.metadata}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc23793",
   "metadata": {},
   "source": [
    "### Transformation into a retriever\n",
    "You can also transform the vector store into a retriever for easier usage in your chains:\n",
    "\n",
    "## Filtered Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "302f7bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* there are cats in the pond [{'topic': 'animals', 'doc_id': 1, 'location': 'pond'}]\n",
      "* the new art exhibit is fascinating [{'topic': 'art', 'doc_id': 5, 'location': 'museum'}]\n",
      "* fresh apples are available at the market [{'topic': 'food', 'doc_id': 3, 'location': 'market'}]\n",
      "* the library hosts a weekly story time for kids [{'topic': 'reading', 'doc_id': 9, 'location': 'library'}]\n",
      "* a new coffee shop opened on Main Street [{'topic': 'food', 'doc_id': 7, 'location': 'Main Street'}]\n",
      "* there are cats in the pond [{'topic': 'animals', 'doc_id': 1, 'location': 'pond'}]\n",
      "* fresh apples are available at the market [{'topic': 'food', 'doc_id': 3, 'location': 'market'}]\n",
      "* the new art exhibit is fascinating [{'topic': 'art', 'doc_id': 5, 'location': 'museum'}]\n",
      "* the library hosts a weekly story time for kids [{'topic': 'reading', 'doc_id': 9, 'location': 'library'}]\n",
      "* a new coffee shop opened on Main Street [{'topic': 'food', 'doc_id': 7, 'location': 'Main Street'}]\n"
     ]
    }
   ],
   "source": [
    "results = vector_store.max_marginal_relevance_search(\n",
    "    \"query about cats\",\n",
    "    k=10,\n",
    "    lambda_mult=0.5,\n",
    "    filter=FilterCondition(\n",
    "        column=\"(metadata->>'doc_id')::int\",\n",
    "        operator=\"in\",\n",
    "        value=[1, 3, 5, 7, 9],\n",
    "    ),\n",
    ")\n",
    "\n",
    "for doc in results:\n",
    "    print(f\"* {doc.page_content} [{doc.metadata}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9970754e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* there are cats in the pond [{'topic': 'animals', 'doc_id': 1, 'location': 'pond'}]\n"
     ]
    }
   ],
   "source": [
    "retriever = vector_store.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 1})\n",
    "results = retriever.invoke(\"kitty\")\n",
    "\n",
    "for doc in results:\n",
    "    print(f\"* {doc.page_content} [{doc.metadata}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c9199c",
   "metadata": {},
   "source": [
    "# Using Postgres for Chat History & Memory Management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441b421c",
   "metadata": {},
   "source": [
    "[Memory](https://langchain-ai.github.io/langgraph/how-tos/memory/add-memory/) is a system that remembers information about previous interactions. For AI agents, memory is crucial because it lets them remember previous interactions, learn from feedback, and adapt to user preferences. \n",
    "\n",
    "This conceptual guide covers two types of memory, based on their recall scope:\n",
    "\n",
    "* [Short-term memory](https://langchain-ai.github.io/langgraph/concepts/memory/#short-term-memory), or [thread](https://langchain-ai.github.io/langgraph/concepts/persistence/#threads)-scoped memory, tracks the ongoing conversation by maintaining message history within a session. LangGraph manages short-term memory as a part of your agent's [state](https://langchain-ai.github.io/langgraph/concepts/low_level/#state). State is persisted to a database using a [checkpointer](https://langchain-ai.github.io/langgraph/concepts/persistence/#checkpoints). \n",
    "\n",
    "* [Long-term memory](https://langchain-ai.github.io/langgraph/concepts/memory/#long-term-memory) stores user-specific or application-level data across sessions and is shared across conversational threads. It can be recalled at any time and in any thread. LangGraph provides [stores](https://langchain-ai.github.io/langgraph/concepts/persistence/#memory-store) to let you save and recall long-term memories.\n",
    "\n",
    "More details [here](https://langchain-ai.github.io/langgraph/concepts/memory/#memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326483cf",
   "metadata": {},
   "source": [
    "## Store & Retrieve Agent Short Term Memory\n",
    "\n",
    "We'll implement helper functions for writing semantic or structured memory, optionally embedding values for similarity recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ba82c785",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.postgres import PostgresSaver\n",
    "from langgraph.store.postgres import PostgresStore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cac7a2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup is done the first time checkpointer and store are used\n",
    "\n",
    "with (\n",
    "    connection_pool.connection() as conn_checkpointer,\n",
    "    connection_pool.connection() as conn_store,\n",
    "):\n",
    "    checkpointer = PostgresSaver(conn_checkpointer)\n",
    "    checkpointer.setup()\n",
    "\n",
    "    store = PostgresStore(conn_store)\n",
    "    store.setup()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ddf3cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "hi! I'm bob\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hi Bob! What can I do for you today?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "what's my name?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Your name is Bob. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langgraph.graph import StateGraph, MessagesState, START\n",
    "\n",
    "model = init_chat_model(model=\"azure_openai:gpt-4.1-mini\")\n",
    "\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    response = model.invoke(state[\"messages\"])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "conn_checkpointer = connection_pool.getconn()\n",
    "checkpointer = PostgresSaver(conn_checkpointer)\n",
    "\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(call_model)\n",
    "builder.add_edge(START, \"call_model\")\n",
    "graph = builder.compile(checkpointer=checkpointer)\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "for chunk in graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"hi! I'm bob\"}]},\n",
    "    config,\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    chunk[\"messages\"][-1].pretty_print()\n",
    "for chunk in graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"what's my name?\"}]},\n",
    "    config,\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4963c0",
   "metadata": {},
   "source": [
    "## Store & Retrieve Agent Long Term Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fb2f4a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hi! Remember: my name is Bob\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hi Bob! I remember your name. What would you like to do today?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "what is my name?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Your name is Bob. How can I assist you today, Bob?\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langgraph.graph import StateGraph, MessagesState, START\n",
    "from langgraph.checkpoint.postgres import PostgresSaver\n",
    "from langgraph.store.postgres import PostgresStore\n",
    "from langgraph.store.base import BaseStore\n",
    "import uuid\n",
    "\n",
    "conn_store = connection_pool.getconn()\n",
    "store = PostgresStore(conn_store)\n",
    "\n",
    "\n",
    "def call_model(\n",
    "    state: MessagesState,\n",
    "    config: RunnableConfig,\n",
    "    *,\n",
    "    store: BaseStore,\n",
    "):\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "    namespace = (\"memories\", user_id)\n",
    "    memories = store.search(namespace, query=str(state[\"messages\"][-1].content))\n",
    "    info = \"\\n\".join([d.value[\"data\"] for d in memories])\n",
    "    system_msg = f\"You are a helpful assistant talking to the user. User info: {info}\"\n",
    "\n",
    "    # Store new memories if the user asks the model to remember\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if \"remember\" in last_message.content.lower():\n",
    "        memory = \"User name is Bob\"\n",
    "        store.put(namespace, str(uuid.uuid4()), {\"data\": memory})\n",
    "\n",
    "    response = model.invoke(\n",
    "        [{\"role\": \"system\", \"content\": system_msg}] + state[\"messages\"]\n",
    "    )\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(call_model)\n",
    "builder.add_edge(START, \"call_model\")\n",
    "\n",
    "graph = builder.compile(\n",
    "    checkpointer=checkpointer,\n",
    "    store=store,\n",
    ")\n",
    "\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "        \"thread_id\": \"1\",\n",
    "        \"user_id\": \"1\",\n",
    "    }\n",
    "}\n",
    "for chunk in graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Hi! Remember: my name is Bob\"}]},\n",
    "    config,\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    chunk[\"messages\"][-1].pretty_print()\n",
    "\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "        \"thread_id\": \"2\",\n",
    "        \"user_id\": \"1\",\n",
    "    }\n",
    "}\n",
    "\n",
    "for chunk in graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"what is my name?\"}]},\n",
    "    config,\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7d3e18",
   "metadata": {},
   "source": [
    "## LangGraph Quick Start Demo using Postgres for Memory and Vector Search\n",
    "Using Postgres for Short Term Memory and Vector Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "324a9236",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "\n",
    "def get_data_from_vector_store(query: str) -> str:\n",
    "    \"\"\"Get data from the vector store.\"\"\"\n",
    "    results = vector_store.similarity_search(query)\n",
    "    return results\n",
    "\n",
    "\n",
    "agent = create_react_agent(\n",
    "    model=model, tools=[get_data_from_vector_store], checkpointer=checkpointer\n",
    ")\n",
    "\n",
    "# Run the agent\n",
    "config = {\"configurable\": {\"thread_id\": \"2\", \"user_id\": \"1\"}}\n",
    "\n",
    "response = agent.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"What does my database say about cats? Make sure you address me with my name\",\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    config,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "00f56993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What does my database say about cats? Make sure you address me with my name\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Bob, your database says that cats are independent pets that often enjoy their own space. If you need more details or have other questions, just let me know!\n"
     ]
    }
   ],
   "source": [
    "for message in response[\"messages\"][-2:]:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f5ff94",
   "metadata": {},
   "source": [
    "## More Examples\n",
    "We have another doc for Long Term Memory and Postgres here: ###### https://python.langchain.com/docs/versions/migrating_memory/long_term_memory_agent/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707a6112",
   "metadata": {},
   "source": [
    "## Summary\n",
    "We consolidated agent state, chat history, semantic memory, and vector search into Azure PostgreSQL, simplifying infra & governance. LangGraph's pluggable checkpointing lets Postgres serve as a durable orchestrator backend. pgvector enables RAG without an external vector DB.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langgraph-azure-postgres-blog (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
